{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298c3b07",
   "metadata": {},
   "source": [
    "# NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a36219",
   "metadata": {},
   "source": [
    " 1. Data acquisition\n",
    " 2. Text Preparation\n",
    "      - Text Cleanup\n",
    "      - Basic Preprocessing\n",
    "      - Advance Preprocessing\n",
    " 3. Feature Engineering\n",
    " 4. Modeling\n",
    "      - Model Building\n",
    "      - Evaluation\n",
    " 5. Deployment\n",
    "      - Deployment\n",
    "      - Monitoring\n",
    "      - Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b1f65",
   "metadata": {},
   "source": [
    "1. **Data Acquisition**\n",
    "    - Data is Available on Company\n",
    "        - In Hand\n",
    "        - On Data Base\n",
    "            - Contact with Data Engineer\n",
    "        - Less Data\n",
    "             - Data Augument (Create a Fake Data)\n",
    "                 - Synonames\n",
    "                 - Bigram Flip\n",
    "                 - Back Translate\n",
    "                 - Add aditional noise\n",
    "    - Data is available outside company\n",
    "        - Use public Dataset\n",
    "            - example Kaggle\n",
    "        - Use webscraping\n",
    "            - using beautifulsoup\n",
    "        - Use API\n",
    "            -  example Rapid API\n",
    "            - library used request\n",
    "            - get data as a JSON format\n",
    "        - Data as a form of pdf\n",
    "        - Data available as a image\n",
    "        - Data available as a audio\n",
    "            - use speech to text library\n",
    "    - Data does not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e3eff",
   "metadata": {},
   "source": [
    "2. **Text Preparation**\n",
    "    - Data cleaning\n",
    "        - HTML tag cleaning\n",
    "        - Emoji cleaning (Unicode Normalization)\n",
    "        - Spelling check (using textblob)\n",
    "    - Basic Preprocessing\n",
    "        - Basic\n",
    "            - Tokenization\n",
    "                - sentence tokenization\n",
    "                - word tokenization\n",
    "        - Optional\n",
    "            - stop word removal\n",
    "            - steaming/lemitization\n",
    "            - removing digits\n",
    "            - lower/upper casing\n",
    "            - language detection\n",
    "    - Advance Preprocessing\n",
    "        - part of speech (POS) tagging\n",
    "        - parsing\n",
    "        - core reference resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38ec2d",
   "metadata": {},
   "source": [
    "#### HTML tag cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef70f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>HTML Element</h2>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HTML Element'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text='<h2>HTML Element</h2>'\n",
    "print(sample_text)\n",
    "print()\n",
    "\n",
    "import re\n",
    "def striphtml(data):\n",
    "    p=re.compile(r'<.*?>')\n",
    "    return p.sub('',data)\n",
    "\n",
    "striphtml(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecae75",
   "metadata": {},
   "source": [
    "#### Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46eadc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe2\\x9c\\x82\\xef\\xb8\\x8f Copy and \\xf0\\x9f\\x93\\x8b Paste Emoji \\xf0\\x9f\\x91\\x8d'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_text=\"âœ‚ï¸ Copy and ðŸ“‹ Paste Emoji ðŸ‘\"\n",
    "emoji_text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab0809",
   "metadata": {},
   "source": [
    "#### Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd40c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\91975\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\91975\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91975\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91975\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b039f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"By how are you , but i am not fine.\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text=\"Hy how aree yoy , but i am not fine.\"\n",
    "from textblob import TextBlob\n",
    "textBlb=TextBlob(incorrect_text)\n",
    "textBlb.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89519613",
   "metadata": {},
   "source": [
    "#### Word Tokenization â€“ Splitting words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde0e337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.', 'You', 'are', 'studying', 'NLP', 'article']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73259473",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization â€“ Splitting sentences in the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d36683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'Welcome to GeeksforGeeks.', 'You are studying NLP article']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957bc8e",
   "metadata": {},
   "source": [
    "3. **Feature Engineering**\n",
    "    - Machine Learning Feature engineering\n",
    "    - Deep Learning Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056fd70",
   "metadata": {},
   "source": [
    "4. **Modeling**\n",
    "    - Modeling selection (based on amount of data and nature of problem)\n",
    "        - Heuristic approach\n",
    "            - used if we have less amount of data (Example spam classifier)\n",
    "        - ML algo\n",
    "            - used if we have moderate amount of data\n",
    "        - DL algo\n",
    "            - used if we have large amount of data (Example BERT)\n",
    "        - Cloud API\n",
    "    - Evaluation (how model perform ?)\n",
    "        - intrinsic\n",
    "        - extrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f141f",
   "metadata": {},
   "source": [
    "5. **Deployment**\n",
    "    - Deployment\n",
    "        - use API for deployment \n",
    "        - chatbot\n",
    "    - Monitoring\n",
    "    - Update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
