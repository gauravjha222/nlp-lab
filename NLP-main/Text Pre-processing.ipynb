{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e08f42",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "- Basic\n",
    "    - Lowercasing\n",
    "    - Remove html tag\n",
    "    - Remove URLs\n",
    "    - Remove Punctuation\n",
    "    - Chat word treatment\n",
    "    - Spelling Correction\n",
    "    - Removing stop words\n",
    "    - Handling Emojis\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "- Advance\n",
    "    - POS tag\n",
    "    - Chunking\n",
    "    - Parsing\n",
    "    - Co-reference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5f102",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f416d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafce6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8323f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeeff222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdab0e0",
   "metadata": {},
   "source": [
    "### Remove html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5396cd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake)\\nthinks there's a zombie in his closet & his parents are \\nfighting all the time.this movie is slower than \\na soap opera... and suddenly, jake decides to become rambo \\nand kill the zombie.ok, first of all when you're \\ngoing to make a film you must decide if its a thriller or a \\ndrama! as a drama the movie is watchable. parents are divorcing \\n& arguing like in real life. and then we have jake with his \\ncloset which totally ruins all the film! i expected to see a \\nboogeyman similar movie, and instead i watched a drama with some \\nmeaningless thriller spots.3 out of 10 just for the \\nwell playing parents & descent dialogs. as for the shots with \\njake: just ignore them.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)\n",
    "text='''basically there's a family where a little boy (jake)\n",
    "thinks there's a zombie in his closet & his parents are \n",
    "fighting all the time.<br /><br />this movie is slower than \n",
    "a soap opera... and suddenly, jake decides to become rambo \n",
    "and kill the zombie.<br /><br />ok, first of all when you're \n",
    "going to make a film you must decide if its a thriller or a \n",
    "drama! as a drama the movie is watchable. parents are divorcing \n",
    "& arguing like in real life. and then we have jake with his \n",
    "closet which totally ruins all the film! i expected to see a \n",
    "boogeyman similar movie, and instead i watched a drama with some \n",
    "meaningless thriller spots.<br /><br />3 out of 10 just for the \n",
    "well playing parents & descent dialogs. as for the shots with \n",
    "jake: just ignore them.'''\n",
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bbab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3324903d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11e571",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc0f8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for my notebook click '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "text1='for my notebook click https://www.kaggle.com/code/saquib7hussain/nlp-pipeline/notebook'\n",
    "remove_url(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd310f12",
   "metadata": {},
   "source": [
    "### Remove Punction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "add5f2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6062261",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf361a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06323933601379395\n"
     ]
    }
   ],
   "source": [
    "''' below code is take much more time for large \n",
    "number of text(rows=50000) hence we used another method'''\n",
    "# method-1\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text\n",
    "text='string. with. Punctuation?'*100000\n",
    "start=time.time()\n",
    "remove_punc(text)\n",
    "time1=time.time() - start\n",
    "print(time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1faea1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0058977603912353516\n"
     ]
    }
   ],
   "source": [
    "# method-2\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "start=time.time()\n",
    "remove_punc1(text)\n",
    "time2=time.time() - start\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9314a2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.722601770626996"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time2 is 10 times faster than time1\n",
    "time1/time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db29c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd0e5649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production The filming technique is very unassuming very oldtimeBBC fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece The actors are extremely well chosen Michael Sheen not only has got all the polari but he has all the voices down pat too You can truly see the seamless editing guided by the references to Williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece A masterful production about one of the great masters of comedy and his life The realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears It plays on our knowledge and our senses particularly with the scenes concerning Orton and Halliwell and the sets particularly of their flat with Halliwells murals decorating every surface are terribly well done'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981c674",
   "metadata": {},
   "source": [
    "### Chat word treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46778b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "chat_conversion(\"IMHO he is the best\")\n",
    "\n",
    "# output: In my humble opinion he is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735080c3",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c6a9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same name'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "incorrect_text='certain conditionas duriing seveal ggenerations arre moodified in the saame namer'\n",
    "textBlb=TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1fa675",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9989184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "remove_stoprwords('certain conditions during severalgenerations are modified in the same name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for apply on whole dataset\n",
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21386254",
   "metadata": {},
   "source": [
    "### Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed508a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "     -------------------------------------- 175.4/175.4 kB 2.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<7.0,>=6.0\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.1/53.1 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171059 sha256=2a0756496f73bfc60c81f108dc6254c9310ff12b0b291f3f718e0a7f9d867530\n",
      "  Stored in directory: c:\\users\\91975\\appdata\\local\\pip\\cache\\wheels\\37\\b1\\70\\d87e2dddea71a019314970e3ea065b63e27b9be29e4a579b13\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, ftfy, clean-text\n",
      "  Attempting uninstall: emoji\n",
      "    Found existing installation: emoji 2.7.0\n",
      "    Uninstalling emoji-2.7.0:\n",
      "      Successfully uninstalled emoji-2.7.0\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aae8e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sample text contains laughing emojis\n"
     ]
    }
   ],
   "source": [
    "# for handle this\n",
    "# 1- remove it\n",
    "# 2- replace with its meaning\n",
    "\n",
    "# 1- remove it\n",
    "#import clean function\n",
    "from cleantext import clean\n",
    "\n",
    "#provide string with emojis\n",
    "text = \"This sample text contains laughing emojis üòÄ üòÉ üòÑ üòÅ üòÜ üòÖ üòÇ ü§£\"\n",
    "\n",
    "#print text after removing the emojis from it\n",
    "print(clean(text, no_emoji=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70422111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :rolling_on_the_floor_laughing:\n"
     ]
    }
   ],
   "source": [
    "# 2- replace with its meaning\n",
    "import emoji\n",
    "print(emoji.demojize('Python is ü§£'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc58055",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a9b3b",
   "metadata": {},
   "source": [
    "Problem wih tokenization\n",
    "- prefix : $(\"\n",
    "- Suffix : km),.!\"\n",
    "- Infix : - -- / _\n",
    "- Exception : let's U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19719550",
   "metadata": {},
   "source": [
    "#### 1. Using the split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32d7d4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "sent1='I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba76982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to the great\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2='I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to the great'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f51669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with split function\n",
    "sent3='I am going to delhi!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f037ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where do think I should go? I have 3 days Holiday']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with split function\n",
    "sent4='where do think I should go? I have 3 days Holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3445f4d",
   "metadata": {},
   "source": [
    "#### 2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1749839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3='I am going to delhi!'\n",
    "tokens=re.findall(\"[\\w']+\",sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66370b0",
   "metadata": {},
   "source": [
    "#### 3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4edbb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a745cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi', '!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1='I am going to delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b5ad5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'Ph.D', 'in', 'A.I']\n",
      "['we', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "sent5='I have Ph.D in A.I'\n",
    "sent6=\"we're here to help! mail us at nks@gmail.com\"\n",
    "sent7='A 5km ride cost $10.50'\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25973a75",
   "metadata": {},
   "source": [
    "#### 4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8de6a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.1-cp310-cp310-win_amd64.whl (12.0 MB)\n",
      "     ---------------------------------------- 12.0/12.0 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.12-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.2/122.2 kB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.9/45.9 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "     ------------------------------------- 481.9/481.9 kB 10.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\91975\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91975\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 confection-0.1.3 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 pathy-0.10.2 preshed-3.0.9 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.1.12 typer-0.9.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af21795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 95, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1042, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 358, in connect\n",
      "    self.sock = conn = self._new_conn()\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 179, in _new_conn\n",
      "    raise ConnectTimeoutError(\n",
      "urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001BD29809660>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 489, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BD29809660>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\spacy\\cli\\_util.py\", line 92, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\click\\core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\spacy\\cli\\download.py\", line 36, in download_cli\n",
      "    download(model, direct, sdist, *ctx.args)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\spacy\\cli\\download.py\", line 70, in download\n",
      "    compatibility = get_compatibility()\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\spacy\\cli\\download.py\", line 94, in get_compatibility\n",
      "    r = requests.get(about.__compatibility__)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\91975\\anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 553, in send\n",
      "    raise ConnectTimeout(e, request=request)\n",
      "requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/compatibility.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001BD29809660>, 'Connection to raw.githubusercontent.com timed out. (connect timeout=None)'))\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9257d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "an\n",
      "example\n",
      "sentence\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp=English()\n",
    "# Text to be tokenized\n",
    "text = \"This is an example sentence.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through the tokens and print them\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08883f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp=English()\n",
    "# Text to be tokenized\n",
    "text='I have Ph.D in A.I'\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through the tokens and print them\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09d5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp=English()\n",
    "# Text to be tokenized\n",
    "text=\"we're here to help! mail us at nks@gmail.com\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through the tokens and print them\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4af13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp=English()\n",
    "# Text to be tokenized\n",
    "text='A 5km ride cost $10.50'\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate through the tokens and print them\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda678f",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f86ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ingrammar , inflection is the modification of a words to\n",
    "express different grammatical categories such as tense,\n",
    "case, voice, aspect, person, number, gender, and mood'''\n",
    "\n",
    "'''Stemming is the process of reducing inflection in words\n",
    "to their root forms such as mapping a group of words to\n",
    "the same stem even if the stem itself is not a valid word \n",
    "in the language it is used in information retreaval'''\n",
    "\n",
    "# library used with the help of nltk\n",
    "# porter streamer (for english) and snow ball streamer (for other language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2201f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c3db99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample=\"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d1b7e",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c354db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming is fast and Lemitization is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4241ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lemmatization , unlike stemming reduces the inflected words \n",
    "properly ensuring that the root word belongs to the language.\n",
    "In lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata)\n",
    "is the cannonical form , dictionary form , or citation form of a set of words'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021982f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n",
    "\t\t'driving', 'died', 'tried', 'feet']\n",
    "for words in list1:\n",
    "\tprint(words + \" ---> \" + wnl.lemmatize(words))\n",
    "\t\n",
    "#> kites ---> kite\n",
    "#> babies ---> baby\n",
    "#> dogs ---> dog\n",
    "#> flying ---> flying\n",
    "#> smiling ---> smiling\n",
    "#> driving ---> driving\n",
    "#> died ---> died\n",
    "#> tried ---> tried\n",
    "#> feet ---> foot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
